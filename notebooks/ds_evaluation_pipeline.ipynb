{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS Evaluation Pipeline: Matching & Metrics\n",
    "\n",
    "**Purpose**: Load pre-computed `.npz` files, run the full matching pipeline (geometric + descriptor + fusion), and evaluate with standard biometric metrics.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Google Drive with `face-auth-data/mast3r_outputs/` (enrollment `.npz`) and `face-auth-data/auth_probes/` (probe `.npz`)\n",
    "- These files are generated by CS-1's `scripts/prepare_public_dataset.py`\n",
    "\n",
    "**No MASt3R model loading needed** — this notebook works entirely with pre-computed data.\n",
    "\n",
    "**GPU**: If a T4 GPU is available, descriptor matching is automatically accelerated.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment Setup\n",
    "# ============================================================\n",
    "# Install dependencies and mount Google Drive.\n",
    "# ============================================================\n",
    "\n",
    "!pip install -q open3d plotly scikit-learn tqdm\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name} ({vram_gb:.1f} GB) — descriptor matching will use GPU\")\n",
    "else:\n",
    "    print(\"No GPU — descriptor matching will use CPU (still works, just slower)\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SHARED_DIR = \"/content/drive/MyDrive/face-auth-data\"\n",
    "ENROLLMENT_DIR = f\"{SHARED_DIR}/mast3r_outputs\"\n",
    "PROBE_DIR = f\"{SHARED_DIR}/auth_probes\"\n",
    "\n",
    "print(f\"\\nEnrollment dir: {ENROLLMENT_DIR}\")\n",
    "print(f\"Probe dir:      {PROBE_DIR}\")\n",
    "print(f\"Enrollment dir exists: {os.path.isdir(ENROLLMENT_DIR)}\")\n",
    "print(f\"Probe dir exists:      {os.path.isdir(PROBE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Clone Repo & Configure Python Path\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "\n",
    "REPO_URL = \"https://github.com/gaelgm03/ai-visual-computing-pbl.git\"\n",
    "BRANCH = \"main\"  # Change to your feature branch if needed\n",
    "REPO_DIR = \"/content/ai-visual-computing-pbl\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone --depth 1 {REPO_URL} {REPO_DIR}\n",
    "\n",
    "%cd {REPO_DIR}\n",
    "!git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "print(f\"\\nRepo ready at {REPO_DIR}, branch: {BRANCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Discover Available .npz Files\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def discover_data(enrollment_dir, probe_dir):\n",
    "    \"\"\"Find all enrollment/probe pairs by matching person names.\"\"\"\n",
    "    enrollments = {}\n",
    "    probes = {}\n",
    "\n",
    "    if os.path.isdir(enrollment_dir):\n",
    "        for f in os.listdir(enrollment_dir):\n",
    "            if f.endswith(\"_enrollment.npz\"):\n",
    "                name = f.replace(\"_enrollment.npz\", \"\")\n",
    "                enrollments[name] = os.path.join(enrollment_dir, f)\n",
    "\n",
    "    if os.path.isdir(probe_dir):\n",
    "        for f in os.listdir(probe_dir):\n",
    "            if f.endswith(\"_probe.npz\"):\n",
    "                name = f.replace(\"_probe.npz\", \"\")\n",
    "                probes[name] = os.path.join(probe_dir, f)\n",
    "\n",
    "    subjects = sorted(set(enrollments.keys()) & set(probes.keys()))\n",
    "    return subjects, enrollments, probes\n",
    "\n",
    "\n",
    "subjects, enrollments, probes = discover_data(ENROLLMENT_DIR, PROBE_DIR)\n",
    "\n",
    "print(f\"Found {len(subjects)} subjects with both enrollment and probe data:\")\n",
    "for s in subjects:\n",
    "    e = np.load(enrollments[s])\n",
    "    p = np.load(probes[s])\n",
    "    print(f\"  {s}: enrollment={e['point_cloud'].shape[0]:,} pts, \"\n",
    "          f\"probe={p['point_cloud'].shape[0]:,} pts\")\n",
    "\n",
    "if not subjects:\n",
    "    print(\"\\nNo matching enrollment/probe pairs found!\")\n",
    "    print(f\"  Check that {ENROLLMENT_DIR} has *_enrollment.npz files\")\n",
    "    print(f\"  and {PROBE_DIR} has *_probe.npz files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Inspect Data Format\n",
    "# ============================================================\n",
    "\n",
    "sample = np.load(enrollments[subjects[0]], allow_pickle=True)\n",
    "print(\"Keys:\", list(sample.keys()))\n",
    "for key in sample.keys():\n",
    "    arr = sample[key]\n",
    "    if hasattr(arr, 'shape'):\n",
    "        print(f\"  {key}: shape={arr.shape}, dtype={arr.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(arr)}\")\n",
    "\n",
    "# Check descriptor normalization\n",
    "desc = sample[\"descriptors\"]\n",
    "norms = np.linalg.norm(desc, axis=1)\n",
    "print(f\"\\nDescriptor norms: min={norms.min():.3f}, max={norms.max():.3f}, mean={norms.mean():.3f}\")\n",
    "if abs(norms.mean() - 1.0) > 0.1:\n",
    "    print(\"Descriptors are NOT unit-normalized — the matcher will normalize internally.\")\n",
    "else:\n",
    "    print(\"Descriptors appear unit-normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Visualize Point Cloud (CS2DS-share.md §4.2)\n",
    "# ============================================================\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def visualize_point_cloud(points, colors=None, title=\"Point Cloud\", max_points=10000):\n",
    "    \"\"\"Interactive 3D point cloud visualization.\"\"\"\n",
    "    if len(points) > max_points:\n",
    "        idx = np.random.choice(len(points), max_points, replace=False)\n",
    "        points = points[idx]\n",
    "        if colors is not None:\n",
    "            colors = colors[idx]\n",
    "\n",
    "    if colors is not None:\n",
    "        color_str = [f'rgb({r},{g},{b})' for r, g, b in colors]\n",
    "    else:\n",
    "        color_str = points[:, 2]\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=points[:, 0], y=points[:, 1], z=points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=1.5, color=color_str, opacity=0.8),\n",
    "    )])\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(aspectmode='data'),\n",
    "        width=800, height=600,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Show enrollment and probe side by side for the first subject\n",
    "person = subjects[0]\n",
    "e_data = np.load(enrollments[person])\n",
    "p_data = np.load(probes[person])\n",
    "\n",
    "fig = visualize_point_cloud(\n",
    "    e_data[\"point_cloud\"], e_data[\"colors\"],\n",
    "    title=f\"{person} — Enrollment ({e_data['point_cloud'].shape[0]:,} pts)\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = visualize_point_cloud(\n",
    "    p_data[\"point_cloud\"], p_data[\"colors\"],\n",
    "    title=f\"{person} — Probe ({p_data['point_cloud'].shape[0]:,} pts)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Initialize Matchers\n",
    "# ============================================================\n",
    "\n",
    "from core.matching.geometric_matcher import ICPGeometricMatcher\n",
    "from core.matching.descriptor_matcher import NNDescriptorMatcher\n",
    "from core.matching.score_fusion import WeightedFusion\n",
    "\n",
    "config = {\n",
    "    \"icp\": {\n",
    "        \"max_iterations\": 50,\n",
    "        \"convergence_threshold\": 1e-6,\n",
    "        \"max_correspondence_distance\": 0.05,\n",
    "    },\n",
    "    \"chamfer_alpha\": 30.0,\n",
    "    \"geometric_subsample\": 10000,\n",
    "    \"descriptor_subsample\": 15000,\n",
    "    \"match_ratio_weight\": 0.4,\n",
    "    \"avg_similarity_weight\": 0.6,\n",
    "    \"geometric_weight\": 0.4,\n",
    "    \"descriptor_weight\": 0.6,\n",
    "    \"accept_threshold\": 0.65,\n",
    "}\n",
    "\n",
    "geo_matcher = ICPGeometricMatcher(config)\n",
    "desc_matcher = NNDescriptorMatcher(config)\n",
    "fusion = WeightedFusion(config)\n",
    "\n",
    "print(\"Matchers initialized:\")\n",
    "print(f\"  Geometric: ICP + Chamfer (subsample={config['geometric_subsample']})\")\n",
    "print(f\"  Descriptor: Reciprocal NN (subsample={config['descriptor_subsample']})\")\n",
    "print(f\"  Fusion: geo={config['geometric_weight']}, desc={config['descriptor_weight']}, threshold={config['accept_threshold']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Single Genuine Pair Test\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "\n",
    "person = subjects[0]\n",
    "enrollment = np.load(enrollments[person], allow_pickle=True)\n",
    "probe = np.load(probes[person], allow_pickle=True)\n",
    "\n",
    "print(f\"=== Genuine pair: {person} vs {person} ===\")\n",
    "\n",
    "t0 = time.time()\n",
    "geo_result = geo_matcher.compare(probe[\"point_cloud\"], enrollment[\"point_cloud\"])\n",
    "t1 = time.time()\n",
    "desc_result = desc_matcher.compare(\n",
    "    probe[\"descriptors\"], enrollment[\"descriptors\"],\n",
    "    probe[\"point_cloud\"], enrollment[\"point_cloud\"],\n",
    ")\n",
    "t2 = time.time()\n",
    "final_result = fusion.fuse(geo_result, desc_result)\n",
    "t3 = time.time()\n",
    "\n",
    "print(f\"  Geometric:  {geo_result.score:.3f}  (Chamfer: {geo_result.details.get('chamfer_distance', 'N/A')})\")\n",
    "print(f\"  Descriptor: {desc_result.score:.3f}  (Match ratio: {desc_result.details.get('match_ratio', 0):.3f}, \"\n",
    "      f\"Avg sim: {desc_result.details.get('avg_cosine_similarity', 0):.3f})\")\n",
    "print(f\"  Fused:      {final_result.score:.3f}  is_match={final_result.is_match}\")\n",
    "print(f\"  Time: geo={t1-t0:.2f}s, desc={t2-t1:.2f}s, fusion={t3-t2:.4f}s\")\n",
    "print(f\"  Backend: desc={desc_result.details.get('backend', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Single Impostor Pair Test\n",
    "# ============================================================\n",
    "\n",
    "if len(subjects) >= 2:\n",
    "    person_a = subjects[0]\n",
    "    person_b = subjects[1]\n",
    "    enrollment_a = np.load(enrollments[person_a], allow_pickle=True)\n",
    "    probe_b = np.load(probes[person_b], allow_pickle=True)\n",
    "\n",
    "    geo_result = geo_matcher.compare(probe_b[\"point_cloud\"], enrollment_a[\"point_cloud\"])\n",
    "    desc_result = desc_matcher.compare(\n",
    "        probe_b[\"descriptors\"], enrollment_a[\"descriptors\"],\n",
    "        probe_b[\"point_cloud\"], enrollment_a[\"point_cloud\"],\n",
    "    )\n",
    "    final_result = fusion.fuse(geo_result, desc_result)\n",
    "\n",
    "    print(f\"=== Impostor pair: {person_b} vs {person_a} ===\")\n",
    "    print(f\"  Geometric:  {geo_result.score:.3f}\")\n",
    "    print(f\"  Descriptor: {desc_result.score:.3f}\")\n",
    "    print(f\"  Fused:      {final_result.score:.3f}  is_match={final_result.is_match}\")\n",
    "else:\n",
    "    print(\"Need at least 2 subjects for impostor test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: All-vs-All Matching\n",
    "# ============================================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "similarities = []\n",
    "labels = []  # 1 = genuine, 0 = impostor\n",
    "pair_info = []\n",
    "\n",
    "# Cache loaded data to avoid re-reading\n",
    "enrollment_cache = {}\n",
    "probe_cache = {}\n",
    "for s in subjects:\n",
    "    enrollment_cache[s] = np.load(enrollments[s], allow_pickle=True)\n",
    "    probe_cache[s] = np.load(probes[s], allow_pickle=True)\n",
    "\n",
    "total_pairs = len(subjects) ** 2\n",
    "print(f\"Running {total_pairs} pairs ({len(subjects)} subjects x {len(subjects)} enrollments)...\")\n",
    "\n",
    "for probe_person in tqdm(subjects, desc=\"Matching\"):\n",
    "    p_data = probe_cache[probe_person]\n",
    "\n",
    "    for enroll_person in subjects:\n",
    "        e_data = enrollment_cache[enroll_person]\n",
    "        is_genuine = int(probe_person == enroll_person)\n",
    "\n",
    "        geo_result = geo_matcher.compare(\n",
    "            p_data[\"point_cloud\"], e_data[\"point_cloud\"]\n",
    "        )\n",
    "        desc_result = desc_matcher.compare(\n",
    "            p_data[\"descriptors\"], e_data[\"descriptors\"],\n",
    "            p_data[\"point_cloud\"], e_data[\"point_cloud\"],\n",
    "        )\n",
    "        final_result = fusion.fuse(geo_result, desc_result)\n",
    "\n",
    "        similarities.append(final_result.score)\n",
    "        labels.append(is_genuine)\n",
    "        pair_info.append({\n",
    "            \"probe\": probe_person,\n",
    "            \"enrollment\": enroll_person,\n",
    "            \"genuine\": is_genuine,\n",
    "            \"geo_score\": geo_result.score,\n",
    "            \"desc_score\": desc_result.score,\n",
    "            \"fused_score\": final_result.score,\n",
    "        })\n",
    "\n",
    "similarities = np.array(similarities)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"\\nTotal pairs: {len(similarities)}\")\n",
    "print(f\"  Genuine:  {labels.sum()} ({labels.mean()*100:.1f}%)\")\n",
    "print(f\"  Impostor: {(1-labels).sum().astype(int)} ({(1-labels).mean()*100:.1f}%)\")\n",
    "print(f\"\\nGenuine score range:  [{similarities[labels==1].min():.3f}, {similarities[labels==1].max():.3f}]\")\n",
    "print(f\"Impostor score range: [{similarities[labels==0].min():.3f}, {similarities[labels==0].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Evaluate with FaceRecognitionEvaluator\n",
    "# ============================================================\n",
    "\n",
    "from core.evaluation import FaceRecognitionEvaluator\n",
    "\n",
    "evaluator = FaceRecognitionEvaluator(threshold=config[\"accept_threshold\"])\n",
    "eval_result = evaluator.evaluate(\n",
    "    similarities=similarities.tolist(),\n",
    "    labels=labels.tolist(),\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"EVALUATION RESULTS (threshold={eval_result.threshold:.2f})\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Accuracy:  {eval_result.accuracy:.3f}\")\n",
    "print(f\"  Precision: {eval_result.precision:.3f}\")\n",
    "print(f\"  Recall:    {eval_result.recall:.3f}\")\n",
    "print(f\"  F1 Score:  {eval_result.f1_score:.3f}\")\n",
    "print(f\"  FAR:       {eval_result.far:.3f}\")\n",
    "print(f\"  TAR:       {eval_result.tar:.3f}\")\n",
    "print(f\"  EER:       {eval_result.eer:.3f} (at threshold={eval_result.eer_threshold:.3f})\")\n",
    "print(f\"  AUC:       {eval_result.auc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Per-Path Score Distributions\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "geo_scores = np.array([p['geo_score'] for p in pair_info])\n",
    "desc_scores = np.array([p['desc_score'] for p in pair_info])\n",
    "fused_scores = np.array([p['fused_score'] for p in pair_info])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, scores, title in zip(\n",
    "    axes,\n",
    "    [geo_scores, desc_scores, fused_scores],\n",
    "    ['Geometric (ICP + Chamfer)', 'Descriptor (Reciprocal NN)', 'Fused (Weighted)'],\n",
    "):\n",
    "    ax.hist(scores[labels == 1], bins=20, alpha=0.7, label='Genuine', color='green')\n",
    "    ax.hist(scores[labels == 0], bins=20, alpha=0.7, label='Impostor', color='red')\n",
    "    ax.axvline(x=config['accept_threshold'], color='black', linestyle='--', label='Threshold')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Score Matrix Heatmap\n",
    "# ============================================================\n",
    "\n",
    "n = len(subjects)\n",
    "score_matrix = fused_scores.reshape(n, n)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=score_matrix,\n",
    "    x=subjects,\n",
    "    y=subjects,\n",
    "    colorscale='RdYlGn',\n",
    "    zmin=0, zmax=1,\n",
    "    text=np.round(score_matrix, 2),\n",
    "    texttemplate=\"%{text}\",\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Matching Score Matrix (Probe vs Enrollment)\",\n",
    "    xaxis_title=\"Enrollment\",\n",
    "    yaxis_title=\"Probe\",\n",
    "    width=max(500, 80 * n),\n",
    "    height=max(400, 70 * n),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Weight Optimization (Grid Search)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import f1_score as sk_f1_score\n",
    "\n",
    "alphas = np.arange(0.0, 1.05, 0.05)\n",
    "results_grid = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    beta = 1.0 - alpha\n",
    "    fused = alpha * geo_scores + beta * desc_scores\n",
    "\n",
    "    best_f1, best_thresh = 0.0, 0.5\n",
    "    for thresh in np.arange(0.1, 0.95, 0.01):\n",
    "        preds = (fused >= thresh).astype(int)\n",
    "        f1_val = sk_f1_score(labels, preds, zero_division=0)\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_thresh = thresh\n",
    "\n",
    "    results_grid.append({\n",
    "        'alpha': round(alpha, 2),\n",
    "        'beta': round(beta, 2),\n",
    "        'best_f1': best_f1,\n",
    "        'best_threshold': round(best_thresh, 2),\n",
    "    })\n",
    "\n",
    "best = max(results_grid, key=lambda x: x['best_f1'])\n",
    "\n",
    "print(f\"{'Alpha':>6} {'Beta':>6} {'Best F1':>8} {'Threshold':>10}\")\n",
    "print(\"-\" * 35)\n",
    "for r in results_grid:\n",
    "    marker = \" <-- BEST\" if r == best else \"\"\n",
    "    print(f\"{r['alpha']:>6.2f} {r['beta']:>6.2f} {r['best_f1']:>8.3f} {r['best_threshold']:>10.2f}{marker}\")\n",
    "\n",
    "print(f\"\\nOptimal: geometric_weight={best['alpha']:.2f}, \"\n",
    "      f\"descriptor_weight={best['beta']:.2f}, \"\n",
    "      f\"threshold={best['best_threshold']:.2f}, \"\n",
    "      f\"F1={best['best_f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Re-evaluate with Optimal Parameters\n",
    "# ============================================================\n",
    "\n",
    "optimal_similarities = best['alpha'] * geo_scores + best['beta'] * desc_scores\n",
    "optimal_evaluator = FaceRecognitionEvaluator(threshold=best['best_threshold'])\n",
    "\n",
    "optimal_result = optimal_evaluator.evaluate(\n",
    "    optimal_similarities.tolist(),\n",
    "    labels.tolist(),\n",
    "    plot=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"OPTIMIZED RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Weights:   geo={best['alpha']:.2f}, desc={best['beta']:.2f}\")\n",
    "print(f\"  Threshold: {best['best_threshold']:.2f}\")\n",
    "print(f\"  Accuracy:  {optimal_result.accuracy:.3f}\")\n",
    "print(f\"  F1 Score:  {optimal_result.f1_score:.3f}\")\n",
    "print(f\"  FAR:       {optimal_result.far:.3f}\")\n",
    "print(f\"  TAR:       {optimal_result.tar:.3f}\")\n",
    "print(f\"  EER:       {optimal_result.eer:.3f}\")\n",
    "print(f\"  AUC:       {optimal_result.auc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Recommended config.yaml Values\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RECOMMENDED CONFIG.YAML VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "matching:\n",
    "  geometric_weight: {best['alpha']:.2f}     # Optimized alpha\n",
    "  descriptor_weight: {best['beta']:.2f}    # Optimized beta\n",
    "  accept_threshold: {best['best_threshold']:.2f}    # Optimized threshold\n",
    "\"\"\")\n",
    "print(f\"Based on {len(subjects)} subjects, {len(similarities)} total pairs\")\n",
    "print(f\"F1={optimal_result.f1_score:.3f}, EER={optimal_result.eer:.3f}, AUC={optimal_result.auc_score:.3f}\")\n",
    "\n",
    "# Save figures to Drive\n",
    "figures_dir = f\"{SHARED_DIR}/evaluation_results/figures\"\n",
    "optimal_evaluator.evaluate(\n",
    "    optimal_similarities.tolist(),\n",
    "    labels.tolist(),\n",
    "    plot=False,\n",
    "    save_dir=figures_dir,\n",
    ")\n",
    "print(f\"\\nFigures saved to {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before Closing: Push to GitHub!\n",
    "\n",
    "Colab sessions are temporary. Push any changes before closing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run before closing:\n",
    "\n",
    "# %cd /content/ai-visual-computing-pbl\n",
    "# !git add -A\n",
    "# !git status\n",
    "# !git commit -m \"feat(ds1): add evaluation results\"\n",
    "# !git push origin {BRANCH}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

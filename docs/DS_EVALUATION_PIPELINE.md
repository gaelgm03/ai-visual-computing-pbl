# DS Evaluation Pipeline Guide

This document provides step-by-step instructions for running the face authentication matching pipeline evaluation on Google Colab.

**How to run**: Create a new empty notebook on Colab and copy-paste each cell's code below in order (same approach as `technical-architecture_whole.md` Appendix C).

---

## Table of Contents

1. [Overview](#1-overview)
2. [Prerequisites](#2-prerequisites)
3. [Google Drive Data Layout](#3-google-drive-data-layout)
4. [.npz File Format](#4-npz-file-format)
5. [Execution Steps](#5-execution-steps)
6. [Interpreting Results](#6-interpreting-results)
7. [Parameter Tuning](#7-parameter-tuning)
8. [Troubleshooting](#8-troubleshooting)
9. [Directions for Accuracy Improvement](#9-directions-for-accuracy-improvement)

---

## 1. Overview

### What This Pipeline Does

Loads pre-computed `.npz` files (3D face point clouds + feature descriptors) generated by CS-1, then runs the full evaluation end-to-end:

```
Load .npz files
    |
3D point cloud visualization (plotly)
    |
Matching algorithms
  |-- Geometric Matcher : ICP alignment + Chamfer distance (3D shape comparison)
  +-- Descriptor Matcher: Reciprocal nearest-neighbor matching (feature vector comparison)
    |
Score Fusion (weighted linear combination)
    |
Evaluation metrics (FAR, TAR, EER, AUC, F1, etc.)
    |
Diagnostic plots (4 charts)
    |
Optimal parameter search (grid search)
```

### No MASt3R Model Loading Required

This pipeline uses only pre-computed `.npz` data. No MASt3R model download (~1 GB) or loading is needed. Runs comfortably on Colab's free tier.

### GPU Usage

When a T4 GPU is available on Colab, the Descriptor Matcher's matrix operations are automatically GPU-accelerated (orders of magnitude faster). Falls back to CPU seamlessly if no GPU is available.

---

## 2. Prerequisites

| Item | Details |
|------|---------|
| Google Colab | Free tier works. GPU runtime recommended (T4) |
| Google Drive | Access to the shared `face-auth-data` folder |
| `.npz` data | Generated by CS-1 via `scripts/prepare_public_dataset.py` and uploaded to Google Drive |

---

## 3. Google Drive Data Layout

Follows the shared folder structure defined in `technical-architecture_whole.md` §14 "Development Workflow & Task Ownership". This pipeline uses files under `mast3r_outputs/archive/`.

```
Google Drive (shared folder)
+-- face-auth-data/
    |-- checkpoints/                     # MASt3R weights (not used by this pipeline)
    |-- datasets/                        # DS: public dataset source images
    |
    |-- mast3r_outputs/
    |   +-- archive/
    |       |-- mast3r_outputs/          # Enrollment .npz files
    |       |   |-- alice_enrollment.npz
    |       |   |-- bob_enrollment.npz
    |       |   +-- ...
    |       +-- auth_probes/             # Authentication probe .npz files
    |           |-- alice_probe.npz
    |           |-- bob_probe.npz
    |           +-- ...
    |
    +-- evaluation_results/              # Output directory (written by this pipeline)
        +-- figures/
```

> **Note**: For shared drives, the path may be `/content/drive/Shareddrives/<drive_name>/` instead of `/content/drive/MyDrive/`. Adjust `SHARED_DIR` in Cell 1 accordingly.

---

## 4. .npz File Format

Each `.npz` file contains the following arrays:

| Key | Shape | Dtype | Description |
|-----|-------|-------|-------------|
| `point_cloud` | `(N, 3)` | `float32` | 3D coordinates (x, y, z) in meters |
| `descriptors` | `(N, 24)` | `float32` | MASt3R feature vectors. **Not L2-normalized** |
| `confidence` | `(N,)` | `float32` | MASt3R confidence. `>1` indicates reliable points (not in [0,1] range) |
| `colors` | `(N, 3)` | `uint8` | RGB color (0-255) |
| `metadata` | scalar | `str` | JSON string (user_id, capture time, frame count, etc.) |

### Important Notes

- **Descriptors are not normalized**: MASt3R output descriptors do not have unit L2 norm. The Descriptor Matcher normalizes them internally.
- **Confidence range**: `confidence` values are positive reals in `[0, +inf)`. Values `>=1.0` are considered reliable (see `config.yaml` `confidence_threshold: 1.5`).
- **Color format**: RGB order (not OpenCV's BGR). Can be used directly for visualization.

---

## 5. Execution Steps

### Preparation: Create a Colab Notebook

1. Go to https://colab.research.google.com
2. Create a new empty notebook
3. **Runtime > Change runtime type > T4 GPU**
4. Copy-paste and run each cell below **one at a time**

> **Reference**: The code is also available in `notebooks/ds_evaluation_pipeline.ipynb`, but since Colab cannot directly execute cloned `.ipynb` files reliably, copy-paste from this document is the recommended approach.

---

### Cell 1: GPU Check + Google Drive Mount + Dependency Install

```python
# ============================================================
# Cell 1: Environment Setup
# ============================================================

!nvidia-smi
!pip install -q open3d plotly scikit-learn tqdm

import torch
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"GPU: {gpu_name} ({vram_gb:.1f} GB) — descriptor matching will use GPU")
else:
    print("No GPU — descriptor matching will use CPU (still works, just slower)")

from google.colab import drive
drive.mount('/content/drive')

import os
SHARED_DIR = "/content/drive/MyDrive/face-auth-data"
ENROLLMENT_DIR = f"{SHARED_DIR}/mast3r_outputs/archive/mast3r_outputs"
PROBE_DIR = f"{SHARED_DIR}/mast3r_outputs/archive/auth_probes"

print(f"\nEnrollment dir: {ENROLLMENT_DIR}")
print(f"Probe dir:      {PROBE_DIR}")
print(f"Enrollment dir exists: {os.path.isdir(ENROLLMENT_DIR)}")
print(f"Probe dir exists:      {os.path.isdir(PROBE_DIR)}")
```

**Expected output:**
```
GPU: Tesla T4 (15.0 GB) — descriptor matching will use GPU
Mounted at /content/drive
Enrollment dir exists: True
Probe dir exists:      True
```

If either directory shows `False`, see [Troubleshooting](#8-troubleshooting).

---

### Cell 2: Clone Repo & Configure Python Path

```python
# ============================================================
# Cell 2: Clone Repo & Configure Python Path
# ============================================================

import sys

REPO_URL = "https://github.com/gaelgm03/ai-visual-computing-pbl.git"
BRANCH = "main"  # Change to your feature branch if needed
REPO_DIR = "/content/ai-visual-computing-pbl"

if not os.path.exists(REPO_DIR):
    !git clone --depth 1 {REPO_URL} {REPO_DIR}

%cd {REPO_DIR}
!git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}

# Appendix C Cell 4: Install project dependencies
!pip install -r requirements.txt

sys.path.insert(0, REPO_DIR)
print(f"\nRepo ready at {REPO_DIR}, branch: {BRANCH}")
```

---

### Cell 3: Discover .npz Files

```python
# ============================================================
# Cell 3: Discover Available .npz Files
# ============================================================

import numpy as np
from pathlib import Path

def discover_data(enrollment_dir, probe_dir):
    """Find all enrollment/probe pairs by matching person names."""
    enrollments = {}
    probes = {}

    if os.path.isdir(enrollment_dir):
        for f in os.listdir(enrollment_dir):
            if f.endswith("_enrollment.npz"):
                name = f.replace("_enrollment.npz", "")
                enrollments[name] = os.path.join(enrollment_dir, f)

    if os.path.isdir(probe_dir):
        for f in os.listdir(probe_dir):
            if f.endswith("_probe.npz"):
                name = f.replace("_probe.npz", "")
                probes[name] = os.path.join(probe_dir, f)

    subjects = sorted(set(enrollments.keys()) & set(probes.keys()))
    return subjects, enrollments, probes


subjects, enrollments, probes = discover_data(ENROLLMENT_DIR, PROBE_DIR)

print(f"Found {len(subjects)} subjects with both enrollment and probe data:")
for s in subjects:
    e = np.load(enrollments[s])
    p = np.load(probes[s])
    print(f"  {s}: enrollment={e['point_cloud'].shape[0]:,} pts, "
          f"probe={p['point_cloud'].shape[0]:,} pts")

if not subjects:
    print("\nNo matching enrollment/probe pairs found!")
    print(f"  Check that {ENROLLMENT_DIR} has *_enrollment.npz files")
    print(f"  and {PROBE_DIR} has *_probe.npz files")
```

---

### Cell 4: Inspect Data Format

```python
# ============================================================
# Cell 4: Inspect Data Format
# ============================================================

sample = np.load(enrollments[subjects[0]], allow_pickle=True)
print("Keys:", list(sample.keys()))
for key in sample.keys():
    arr = sample[key]
    if hasattr(arr, 'shape'):
        print(f"  {key}: shape={arr.shape}, dtype={arr.dtype}")
    else:
        print(f"  {key}: {type(arr)}")

# Check descriptor normalization
desc = sample["descriptors"]
norms = np.linalg.norm(desc, axis=1)
print(f"\nDescriptor norms: min={norms.min():.3f}, max={norms.max():.3f}, mean={norms.mean():.3f}")
if abs(norms.mean() - 1.0) > 0.1:
    print("Descriptors are NOT unit-normalized — the matcher will normalize internally.")
else:
    print("Descriptors appear unit-normalized.")
```

---

### Cell 5: Visualize Point Clouds

```python
# ============================================================
# Cell 5: Visualize Point Cloud (CS2DS-share.md S4.2)
# ============================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots

def visualize_point_cloud(points, colors=None, title="Point Cloud", max_points=10000):
    """Interactive 3D point cloud visualization."""
    if len(points) > max_points:
        idx = np.random.choice(len(points), max_points, replace=False)
        points = points[idx]
        if colors is not None:
            colors = colors[idx]

    if colors is not None:
        color_str = [f'rgb({r},{g},{b})' for r, g, b in colors]
    else:
        color_str = points[:, 2]

    fig = go.Figure(data=[go.Scatter3d(
        x=points[:, 0], y=points[:, 1], z=points[:, 2],
        mode='markers',
        marker=dict(size=1.5, color=color_str, opacity=0.8),
    )])
    fig.update_layout(
        title=title,
        scene=dict(aspectmode='data'),
        width=800, height=600,
    )
    return fig


# Show enrollment and probe for the first subject
person = subjects[0]
e_data = np.load(enrollments[person])
p_data = np.load(probes[person])

fig = visualize_point_cloud(
    e_data["point_cloud"], e_data["colors"],
    title=f"{person} — Enrollment ({e_data['point_cloud'].shape[0]:,} pts)"
)
fig.show()

fig = visualize_point_cloud(
    p_data["point_cloud"], p_data["colors"],
    title=f"{person} — Probe ({p_data['point_cloud'].shape[0]:,} pts)"
)
fig.show()
```

Use mouse drag to rotate, scroll to zoom. Verify that the face shape is recognizable.

---

### Cell 6: Initialize Matchers

```python
# ============================================================
# Cell 6: Initialize Matchers
# ============================================================

from core.matching.geometric_matcher import ICPGeometricMatcher
from core.matching.descriptor_matcher import NNDescriptorMatcher
from core.matching.score_fusion import WeightedFusion

config = {
    "icp": {
        "max_iterations": 50,
        "convergence_threshold": 1e-6,
        "max_correspondence_distance": 0.05,
    },
    "chamfer_alpha": 30.0,
    "geometric_subsample": 10000,
    "descriptor_subsample": 15000,
    "match_ratio_weight": 0.4,
    "avg_similarity_weight": 0.6,
    "geometric_weight": 0.4,
    "descriptor_weight": 0.6,
    "accept_threshold": 0.65,
}

geo_matcher = ICPGeometricMatcher(config)
desc_matcher = NNDescriptorMatcher(config)
fusion = WeightedFusion(config)

print("Matchers initialized:")
print(f"  Geometric: ICP + Chamfer (subsample={config['geometric_subsample']})")
print(f"  Descriptor: Reciprocal NN (subsample={config['descriptor_subsample']})")
print(f"  Fusion: geo={config['geometric_weight']}, desc={config['descriptor_weight']}, threshold={config['accept_threshold']}")
```

Matcher roles:

| Matcher | Method | Input | Output |
|---------|--------|-------|--------|
| `ICPGeometricMatcher` | ICP alignment > Chamfer distance > `exp(-alpha*d)` | Two point clouds `(N,3)` | Score [0, 1] |
| `NNDescriptorMatcher` | L2-normalize > Reciprocal NN > match_ratio + cosine_sim | Two descriptor sets `(N,24)` | Score [0, 1] |
| `WeightedFusion` | `0.4*geo + 0.6*desc` > threshold | Two scores above | Final score + accept/reject |

---

### Cell 7: Genuine Pair Test (Same Person)

```python
# ============================================================
# Cell 7: Single Genuine Pair Test
# ============================================================

import time

person = subjects[0]
enrollment = np.load(enrollments[person], allow_pickle=True)
probe = np.load(probes[person], allow_pickle=True)

print(f"=== Genuine pair: {person} vs {person} ===")

t0 = time.time()
geo_result = geo_matcher.compare(probe["point_cloud"], enrollment["point_cloud"])
t1 = time.time()
desc_result = desc_matcher.compare(
    probe["descriptors"], enrollment["descriptors"],
    probe["point_cloud"], enrollment["point_cloud"],
)
t2 = time.time()
final_result = fusion.fuse(geo_result, desc_result)
t3 = time.time()

print(f"  Geometric:  {geo_result.score:.3f}  (Chamfer: {geo_result.details.get('chamfer_distance', 'N/A')})")
print(f"  Descriptor: {desc_result.score:.3f}  (Match ratio: {desc_result.details.get('match_ratio', 0):.3f}, "
      f"Avg sim: {desc_result.details.get('avg_cosine_similarity', 0):.3f})")
print(f"  Fused:      {final_result.score:.3f}  is_match={final_result.is_match}")
print(f"  Time: geo={t1-t0:.2f}s, desc={t2-t1:.2f}s, fusion={t3-t2:.4f}s")
print(f"  Backend: desc={desc_result.details.get('backend', 'unknown')}")
```

**Expected result**: Fused >= 0.65, `is_match=True`

---

### Cell 8: Impostor Pair Test (Different Person)

```python
# ============================================================
# Cell 8: Single Impostor Pair Test
# ============================================================

if len(subjects) >= 2:
    person_a = subjects[0]
    person_b = subjects[1]
    enrollment_a = np.load(enrollments[person_a], allow_pickle=True)
    probe_b = np.load(probes[person_b], allow_pickle=True)

    geo_result = geo_matcher.compare(probe_b["point_cloud"], enrollment_a["point_cloud"])
    desc_result = desc_matcher.compare(
        probe_b["descriptors"], enrollment_a["descriptors"],
        probe_b["point_cloud"], enrollment_a["point_cloud"],
    )
    final_result = fusion.fuse(geo_result, desc_result)

    print(f"=== Impostor pair: {person_b} vs {person_a} ===")
    print(f"  Geometric:  {geo_result.score:.3f}")
    print(f"  Descriptor: {desc_result.score:.3f}")
    print(f"  Fused:      {final_result.score:.3f}  is_match={final_result.is_match}")
else:
    print("Need at least 2 subjects for impostor test")
```

**Expected result**: Fused < 0.65, `is_match=False`

---

### Cell 9: All-vs-All Matching

```python
# ============================================================
# Cell 9: All-vs-All Matching
# ============================================================

from tqdm import tqdm

similarities = []
labels = []  # 1 = genuine, 0 = impostor
pair_info = []

# Cache loaded data to avoid re-reading
enrollment_cache = {}
probe_cache = {}
for s in subjects:
    enrollment_cache[s] = np.load(enrollments[s], allow_pickle=True)
    probe_cache[s] = np.load(probes[s], allow_pickle=True)

total_pairs = len(subjects) ** 2
print(f"Running {total_pairs} pairs ({len(subjects)} subjects x {len(subjects)} enrollments)...")

for probe_person in tqdm(subjects, desc="Matching"):
    p_data = probe_cache[probe_person]

    for enroll_person in subjects:
        e_data = enrollment_cache[enroll_person]
        is_genuine = int(probe_person == enroll_person)

        geo_result = geo_matcher.compare(
            p_data["point_cloud"], e_data["point_cloud"]
        )
        desc_result = desc_matcher.compare(
            p_data["descriptors"], e_data["descriptors"],
            p_data["point_cloud"], e_data["point_cloud"],
        )
        final_result = fusion.fuse(geo_result, desc_result)

        similarities.append(final_result.score)
        labels.append(is_genuine)
        pair_info.append({
            "probe": probe_person,
            "enrollment": enroll_person,
            "genuine": is_genuine,
            "geo_score": geo_result.score,
            "desc_score": desc_result.score,
            "fused_score": final_result.score,
        })

similarities = np.array(similarities)
labels = np.array(labels)

print(f"\nTotal pairs: {len(similarities)}")
print(f"  Genuine:  {labels.sum()} ({labels.mean()*100:.1f}%)")
print(f"  Impostor: {(1-labels).sum().astype(int)} ({(1-labels).mean()*100:.1f}%)")
print(f"\nGenuine score range:  [{similarities[labels==1].min():.3f}, {similarities[labels==1].max():.3f}]")
print(f"Impostor score range: [{similarities[labels==0].min():.3f}, {similarities[labels==0].max():.3f}]")
```

**Estimated time**: 5 subjects (25 pairs) ~1-3 min with GPU, ~5-10 min on CPU.

---

### Cell 10: Evaluation Metrics

```python
# ============================================================
# Cell 10: Evaluate with FaceRecognitionEvaluator
# ============================================================

from core.evaluation import FaceRecognitionEvaluator

evaluator = FaceRecognitionEvaluator(threshold=config["accept_threshold"])
eval_result = evaluator.evaluate(
    similarities=similarities.tolist(),
    labels=labels.tolist(),
    plot=True,
)

print(f"\n{'='*50}")
print(f"EVALUATION RESULTS (threshold={eval_result.threshold:.2f})")
print(f"{'='*50}")
print(f"  Accuracy:  {eval_result.accuracy:.3f}")
print(f"  Precision: {eval_result.precision:.3f}")
print(f"  Recall:    {eval_result.recall:.3f}")
print(f"  F1 Score:  {eval_result.f1_score:.3f}")
print(f"  FAR:       {eval_result.far:.3f}")
print(f"  TAR:       {eval_result.tar:.3f}")
print(f"  EER:       {eval_result.eer:.3f} (at threshold={eval_result.eer_threshold:.3f})")
print(f"  AUC:       {eval_result.auc_score:.3f}")
```

Four diagnostic plots are generated automatically (see [Interpreting Results](#6-interpreting-results)).

| Metric | Description | Ideal |
|--------|-------------|-------|
| Accuracy | Overall correct rate | 1.0 |
| Precision | How accurate "accept" decisions are | 1.0 |
| Recall | Fraction of genuine pairs correctly accepted | 1.0 |
| F1 Score | Harmonic mean of Precision and Recall | 1.0 |
| FAR | Fraction of impostors incorrectly accepted | 0.0 |
| TAR | Fraction of genuine pairs correctly accepted | 1.0 |
| EER | Error rate where FAR = FRR | 0.0 |
| AUC | Area Under ROC Curve | 1.0 |

---

### Cell 11: Per-Path Score Distributions

```python
# ============================================================
# Cell 11: Per-Path Score Distributions
# ============================================================

import matplotlib.pyplot as plt

geo_scores = np.array([p['geo_score'] for p in pair_info])
desc_scores = np.array([p['desc_score'] for p in pair_info])
fused_scores = np.array([p['fused_score'] for p in pair_info])

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, scores, title in zip(
    axes,
    [geo_scores, desc_scores, fused_scores],
    ['Geometric (ICP + Chamfer)', 'Descriptor (Reciprocal NN)', 'Fused (Weighted)'],
):
    ax.hist(scores[labels == 1], bins=20, alpha=0.7, label='Genuine', color='green')
    ax.hist(scores[labels == 0], bins=20, alpha=0.7, label='Impostor', color='red')
    ax.axvline(x=config['accept_threshold'], color='black', linestyle='--', label='Threshold')
    ax.set_title(title)
    ax.set_xlabel('Score')
    ax.set_ylabel('Count')
    ax.legend()

plt.tight_layout()
plt.show()
```

Higher separation between Genuine (green) and Impostor (red) distributions indicates better discriminative ability.

---

### Cell 12: Score Matrix Heatmap

```python
# ============================================================
# Cell 12: Score Matrix Heatmap
# ============================================================

n = len(subjects)
score_matrix = fused_scores.reshape(n, n)

fig = go.Figure(data=go.Heatmap(
    z=score_matrix,
    x=subjects,
    y=subjects,
    colorscale='RdYlGn',
    zmin=0, zmax=1,
    text=np.round(score_matrix, 2),
    texttemplate="%{text}",
))
fig.update_layout(
    title="Matching Score Matrix (Probe vs Enrollment)",
    xaxis_title="Enrollment",
    yaxis_title="Probe",
    width=max(500, 80 * n),
    height=max(400, 70 * n),
)
fig.show()
```

Diagonal cells (same person) should show high scores (green); off-diagonal (different person) should show low scores (red).

---

### Cell 13: Weight Optimization (Grid Search)

```python
# ============================================================
# Cell 13: Weight Optimization (Grid Search)
# ============================================================

from sklearn.metrics import f1_score as sk_f1_score

alphas = np.arange(0.0, 1.05, 0.05)
results_grid = []

for alpha in alphas:
    beta = 1.0 - alpha
    fused = alpha * geo_scores + beta * desc_scores

    best_f1, best_thresh = 0.0, 0.5
    for thresh in np.arange(0.1, 0.95, 0.01):
        preds = (fused >= thresh).astype(int)
        f1_val = sk_f1_score(labels, preds, zero_division=0)
        if f1_val > best_f1:
            best_f1 = f1_val
            best_thresh = thresh

    results_grid.append({
        'alpha': round(alpha, 2),
        'beta': round(beta, 2),
        'best_f1': best_f1,
        'best_threshold': round(best_thresh, 2),
    })

best = max(results_grid, key=lambda x: x['best_f1'])

print(f"{'Alpha':>6} {'Beta':>6} {'Best F1':>8} {'Threshold':>10}")
print("-" * 35)
for r in results_grid:
    marker = " <-- BEST" if r == best else ""
    print(f"{r['alpha']:>6.2f} {r['beta']:>6.2f} {r['best_f1']:>8.3f} {r['best_threshold']:>10.2f}{marker}")

print(f"\nOptimal: geometric_weight={best['alpha']:.2f}, "
      f"descriptor_weight={best['beta']:.2f}, "
      f"threshold={best['best_threshold']:.2f}, "
      f"F1={best['best_f1']:.3f}")
```

---

### Cell 14: Re-evaluate with Optimal Parameters

```python
# ============================================================
# Cell 14: Re-evaluate with Optimal Parameters
# ============================================================

optimal_similarities = best['alpha'] * geo_scores + best['beta'] * desc_scores
optimal_evaluator = FaceRecognitionEvaluator(threshold=best['best_threshold'])

optimal_result = optimal_evaluator.evaluate(
    optimal_similarities.tolist(),
    labels.tolist(),
    plot=True,
)

print(f"\n{'='*50}")
print(f"OPTIMIZED RESULTS")
print(f"{'='*50}")
print(f"  Weights:   geo={best['alpha']:.2f}, desc={best['beta']:.2f}")
print(f"  Threshold: {best['best_threshold']:.2f}")
print(f"  Accuracy:  {optimal_result.accuracy:.3f}")
print(f"  F1 Score:  {optimal_result.f1_score:.3f}")
print(f"  FAR:       {optimal_result.far:.3f}")
print(f"  TAR:       {optimal_result.tar:.3f}")
print(f"  EER:       {optimal_result.eer:.3f}")
print(f"  AUC:       {optimal_result.auc_score:.3f}")
```

---

### Cell 15: Recommended config.yaml Values + Save Results

```python
# ============================================================
# Cell 15: Recommended config.yaml Values
# ============================================================

print("=" * 60)
print("RECOMMENDED CONFIG.YAML VALUES")
print("=" * 60)
print(f"""
matching:
  geometric_weight: {best['alpha']:.2f}     # Optimized alpha
  descriptor_weight: {best['beta']:.2f}    # Optimized beta
  accept_threshold: {best['best_threshold']:.2f}    # Optimized threshold
""")
print(f"Based on {len(subjects)} subjects, {len(similarities)} total pairs")
print(f"F1={optimal_result.f1_score:.3f}, EER={optimal_result.eer:.3f}, AUC={optimal_result.auc_score:.3f}")

# Save figures to Drive
figures_dir = f"{SHARED_DIR}/evaluation_results/figures"
optimal_evaluator.evaluate(
    optimal_similarities.tolist(),
    labels.tolist(),
    plot=False,
    save_dir=figures_dir,
)
print(f"\nFigures saved to {figures_dir}")
```

Copy the output values into the `matching` section of `config.yaml` to apply them to the production system.

---

## 6. Interpreting Results

### Four Diagnostic Plots

#### 1. Similarity Score Distribution
- **Green**: Genuine (same person), **Red**: Impostor (different person), **Black dashed**: Threshold
- **Good result**: Green and red distributions do not overlap

#### 2. Confusion Matrix
- **TN**: Impostor correctly rejected, **FP**: Impostor incorrectly accepted (FAR)
- **FN**: Genuine incorrectly rejected (FRR), **TP**: Genuine correctly accepted (TAR)

#### 3. ROC Curve
- **Ideal**: Curve hugs the top-left corner (AUC = 1.0)
- **Dashed line**: Random guessing (AUC = 0.5)

#### 4. Metrics Summary (Bar Chart)
- Bar chart of all 6 key metrics at a glance

---

## 7. Parameter Tuning

Adjustable via the `config` dictionary in Cell 6.

| Parameter | Default | Description |
|-----------|---------|-------------|
| `icp.max_iterations` | `50` | ICP iteration count. More = better accuracy, slower |
| `icp.max_correspondence_distance` | `0.05` | Max correspondence distance (meters) |
| `chamfer_alpha` | `30.0` | Decay factor in `exp(-alpha*d)` |
| `geometric_subsample` | `10000` | Point cloud subsample limit |
| `match_ratio_weight` | `0.4` | Weight for match ratio in descriptor score |
| `avg_similarity_weight` | `0.6` | Weight for average cosine similarity |
| `descriptor_subsample` | `15000` | Descriptor subsample limit |
| `geometric_weight` | `0.4` | Geometric score weight (alpha) |
| `descriptor_weight` | `0.6` | Descriptor score weight (beta, alpha+beta=1) |
| `accept_threshold` | `0.65` | Accept/reject threshold |

> **Tip**: Cell 13's grid search automatically finds optimal alpha, beta, and threshold values.

---

## 8. Troubleshooting

### Google Drive Directory Not Found (`exists: False`)

**Solution**:
1. Right-click the `face-auth-data` folder in Google Drive > "Add shortcut to Drive" > place in My Drive root
2. Alternative: Directly modify `SHARED_DIR` in Cell 1

```python
# For shared drives
SHARED_DIR = "/content/drive/Shareddrives/YourDriveName/face-auth-data"
```

### `No matching enrollment/probe pairs found!`

**Check**:
- Enrollment files must be named `<name>_enrollment.npz`, probes `<name>_probe.npz`
- The `<name>` portion must match exactly (case-sensitive)

### `open3d` Installation Fails

The pipeline works without open3d. The Geometric Matcher automatically falls back to scipy SVD-based ICP.

### GPU Errors

The Descriptor Matcher has a built-in CPU fallback for `RuntimeError`. Check the `backend` field in output:
- `gpu_matmul`: Using GPU
- `cpu_kdtree`: Using CPU
- `cpu_kdtree (gpu_fallback)`: Fell back to CPU after GPU attempt

### Out of Memory (OOM)

Reduce subsample limits: `geometric_subsample: 5000`, `descriptor_subsample: 8000`

### Descriptor Score is 0.000

**Cause**: The `descriptors` array in the `.npz` may contain NaN/Inf values. These can arise from MASt3R's FP16 inference, image edges, or padding regions. The `descriptor_matcher.py` `_filter_invalid()` method removes these automatically, but if fewer than 5 valid descriptors remain after filtering, the score becomes 0.0.

**Verification**:
```python
desc = np.load("path/to/file.npz")["descriptors"]
print(f"NaN: {np.isnan(desc).any()}, Inf: {np.isinf(desc).any()}")
print(f"Valid rows: {np.isfinite(desc).all(axis=1).sum()} / {len(desc)}")
```

**Fix**: Regenerate the `.npz` files (upstream `_extract_aligned_descriptors` now includes NaN/Inf filtering). If the `sparse_ga.py` descriptor cache patch is applied during regeneration, descriptors are stored in 1:1 correspondence with the 3D point cloud and go through the same spatial filters (dedup/outlier/cluster).

### sparse_ga.py Patch (Descriptor Support)

When regenerating `.npz` files, the following 4 lines must be added to the `forward_mast3r()` function in `third_party/mast3r/mast3r/cloud_opt/sparse_ga.py` (manual patch, similar to the existing `.get()` patch):

```python
# Add after line 594 (after torch.save(to_cpu((X22, ...)))):

            # save per-view descriptors for downstream extraction (face-auth patch)
            desc_path1 = cache_path + f'/desc/{idx1}/{idx2}.pth'
            desc_path2 = cache_path + f'/desc/{idx2}/{idx1}.pth'
            torch.save(to_cpu(descs[0]), mkdir_for(desc_path1))  # desc11: img1's self-desc
            torch.save(to_cpu(descs[2]), mkdir_for(desc_path2))  # desc22: img2's self-desc
```

This patch enables `global_alignment.py` to load per-view descriptors from the forward pass cache, pairing them with `get_dense_pts3d()` output by pixel index for 1:1 correspondence.

### All Matching Scores Are 0.0

Check the Cell 4 output for data shapes. Likely causes: empty point cloud (`< 10 points`) or descriptor dimension mismatch.

---

## 9. Directions for Accuracy Improvement

The grid search in Cell 13 exhausts the linear weight/threshold optimization space. However, other improvement avenues remain. The following are organized by expected impact.

### Diagnosis

If AUC is near 0.56 and EER near 0.47, the genuine and impostor score distributions overlap almost completely. This is not a threshold issue — it means **the features themselves lack discriminative power**.

If the grid search yields `descriptor_weight=0.00` as optimal, MASt3R descriptors are not contributing to identity discrimination. MASt3R descriptors are designed for **inter-view correspondence within the same scene**, not for **distinguishing between different identities** (CS2DS-share.md §5.2 assumes "descriptors are more identity-discriminative", but this assumption may not hold in practice).

### Level 1: Downstream Improvements (Matching Algorithm Changes Only, No .npz Regeneration)

#### a) Tune `chamfer_alpha`

The current distance-to-score conversion uses `score = exp(-30.0 * chamfer_distance)`. If genuine Chamfer distances cluster around 0.02-0.04, alpha=30 compresses scores to 0.37-0.55, making it hard to exceed any reasonable threshold.

Add a `chamfer_alpha` search axis to the Cell 13 grid search:

```python
# Example: search chamfer_alpha alongside fusion weights
for alpha_geo in [5, 10, 15, 20, 30, 50]:
    # Recompute geo scores from raw Chamfer distances
    adjusted_geo = np.exp(-alpha_geo * chamfer_distances)
    for alpha in alphas:
        fused = alpha * adjusted_geo + (1-alpha) * desc_scores
        # ... F1 optimization ...
```

#### b) Global Descriptor Comparison (Alternative to Reciprocal NN)

The current reciprocal NN matching performs point-wise correspondence, which may not be effective when MASt3R descriptors are weak at identity discrimination. Statistical approaches may work better:

- **Mean descriptor cosine similarity**: Compute the mean descriptor vector as a "whole-face signature" and compare probe vs. enrollment mean vectors directly
- **Distribution comparison**: Compare the shape of descriptor distributions (same person should have similar feature distributions across the face surface)

#### c) Score Normalization

If geometric/descriptor scores are concentrated in a narrow range, z-score or min-max normalization can spread the range and make thresholding more effective.

### Level 2: Upstream Improvements (Requires .npz Regeneration)

#### d) Increase Authentication Frame Count

Auth probes currently use 4 frames for reconstruction, but MASt3R reconstruction quality degrades significantly with few frames. Increasing from 4 to 8-12 frames should substantially improve enrollment-probe shape agreement for the same person.

#### e) Landmark-Based Pre-Alignment

The current PCA-then-ICP alignment is generic, not face-specific. Pre-aligning using 3D landmarks (nose tip, eye corners) before running ICP would reduce the risk of ICP converging to a local optimum.

CS2DS-share.md's "Optional Directions to Explore" mentions:
> "Per-region analysis: are certain facial areas more reliable?"

This is not yet implemented, but matching within facial sub-regions (nose, eyes, mouth) and assigning higher weights to more discriminative regions could be an effective approach.

### Level 3: Architectural Considerations

MASt3R features are designed for 3D reconstruction view-correspondence, not identity discrimination. To fundamentally improve AUC, the following approaches may be considered:

- **Face-specific feature extractors** (ArcFace, CosFace, etc.) used alongside MASt3R
- **Metric learning** on MASt3R's descriptor space (training to pull same-person descriptors together and push different-person descriptors apart)
- **Signed Distance Function (SDF)** representations for more robust face shape comparison

These are outside the current design scope (CS2DS-share.md) but are noted as potential future directions.

---

## Related Documents

- `technical-architecture_whole.md` — Full system architecture (Appendix C: Colab environment setup)
- `CS2DS-share.md` — Matching algorithm specification (§5.1-5.3, §10)
- `config.yaml` — System configuration (`matching` section)
- `core/matching/interfaces.py` — Abstract matcher interface definitions
